<p><b>Stand-Alone Self-Attention in Vision Models</b><br><i>Niki Parmar (Google) &middot; Prajit Ramachandran (Google Brain) &middot; Ashish Vaswani (Google Brain) &middot; Irwan Bello (Google) &middot; Anselm Levskaya (Google) &middot; Jon Shlens (Google Research)</i></p>
<p><b>High Fidelity Video Prediction with Large Neural Nets</b><br><i>Ruben Villegas (Adobe Research / U. Michigan) &middot; Arkanath Pathak (Google) &middot; Harini Kannan (Google Brain) &middot; Honglak Lee (Google / U. Michigan) &middot; Dumitru Erhan (Google Brain) &middot; Quoc V Le (Google)</i></p>
<p><b>Unsupervised learning of object structure and dynamics from videos</b><br><i>Matthias Minderer (Google Research) &middot; Chen Sun (Google Research) &middot; Ruben Villegas (Adobe Research / U. Michigan) &middot; Forrester Cole (Google Research) &middot; Kevin P Murphy (Google) &middot; Honglak Lee (Google Brain)</i></p>
<p><b>TensorPipe: Easy Scaling with Micro-Batch Pipeline Parallelism</b><br><i>Yanping Huang (Google Brain) &middot; Youlong Cheng (Google) &middot; Ankur Bapna (Google) &middot; Orhan Firat (Google) &middot; Dehao Chen (Google) &middot; Mia Chen (Google Brain) &middot; HyoukJoong Lee (Google) &middot; Jiquan Ngiam (Google Brain) &middot; Quoc V Le (Google) &middot; Yonghui Wu (Google) &middot; zhifeng Chen (Google Brain)</i></p>
<p><b>Noise-tolerant fair classification</b><br><i>Alex Lamy (Columbia University) &middot; Ziyuan Zhong (Columbia University) &middot; Aditya Menon (Google) &middot; Nakul Verma (Columbia University)</i></p>
<p><b>Joint-task Self-supervised Learning for Temporal Correspondence</b><br><i>xueting li (uc merced) &middot; Sifei Liu (NVIDIA) &middot; Shalini De Mello (NVIDIA) &middot; Xiaolong Wang (CMU) &middot; Jan Kautz (NVIDIA) &middot; Ming-Hsuan Yang (UC Merced / Google)</i></p>
<p><b>Experience Replay for Continual Learning</b><br><i>David Rolnick (UPenn) &middot; Arun Ahuja (DeepMind) &middot; Jonathan Schwarz (DeepMind) &middot; Timothy Lillicrap (Google DeepMind) &middot; Gregory Wayne (Google DeepMind)</i></p>
<p><b>No Pressure! Addressing the Problem of Local Minima in Manifold Learning Algorithms</b><br><i>Max Vladymyrov (Google)</i></p>
<p><b>Saccader: Accurate, Interpretable Image Classification with Hard Attention</b><br><i>Gamaleldin Elsayed (Google Brain) &middot; Simon Kornblith (Google Brain) &middot; Quoc V Le (Google)</i></p>
<p><b>Deep Learning without Weight Transport</b><br><i>Mohamed Akrout (University of Toronto) &middot; Collin Wilson (University of Toronto) &middot; Peter Humphreys (Google) &middot; Timothy Lillicrap (Google DeepMind) &middot; Douglas Tweed (University of Toronto)</i></p>
<p><b>Secretary Ranking with Minimal Inversions</b><br><i>Sepehr Assadi (Princeton University) &middot; Eric Balkanski (Harvard University) &middot; Renato Leme (Google Research)</i></p>
<p><b>CondConv: Conditionally Parameterized Convolutions for Efficient Inference</b><br><i>Brandon Yang (Google Brain) &middot; Gabriel Bender (Google Brain) &middot; Quoc V Le (Google) &middot; Jiquan Ngiam (Google Brain)</i></p>
<p><b>Sampling Sketches for Concave Sublinear Functions of Frequencies</b><br><i>Edith Cohen (Google) &middot; Ofir Geri (Stanford University)</i></p>
<p><b>Strategizing against No-regret Learners</b><br><i>Yuan Deng (Duke University) &middot; Jon Schneider (Google Research) &middot; Balasubramanian Sivan (Google Research)</i></p>
<p><b>Quadratic Video Interpolation</b><br><i>Xiangyu Xu (Tsinghua University) &middot; Li Si-Yao (Beijing Normal University) &middot; Wenxiu Sun (SenseTime Research) &middot; Qian Yin (Beijing Normal University) &middot; Ming-Hsuan Yang (UC Merced / Google)</i></p>
<p><b>Online Stochastic Shortest Path with Bandit Feedback and Unknown Transition Function</b><br><i>Aviv Rosenberg (Tel Aviv University) &middot; Yishay Mansour (Tel Aviv University / Google)</i></p>
<p><b>DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections</b><br><i>Ofir Nachum (Google Brain) &middot; Yinlam Chow (DeepMind) &middot; Bo Dai (Google Brain) &middot; Lihong Li (Google Brain)</i></p>
<p><b>Multiview Aggregation for Learning Category-Specific Shape Reconstruction</b><br><i>Srinath Sridhar (Stanford University) &middot; Davis Rempe (Stanford University) &middot; Julien Valentin (Google) &middot; Bouaziz Sofien () &middot; Leonidas J Guibas (stanford.edu)</i></p>
<p><b>Learning Transferable Graph Exploration</b><br><i>Hanjun Dai (Georgia Tech) &middot; Yujia Li (DeepMind) &middot; Chenglong Wang (University of Washington) &middot; Rishabh Singh (Google Brain) &middot; Po-Sen Huang (DeepMind) &middot; Pushmeet Kohli (DeepMind)</i></p>
<p><b>Locally Private Gaussian Estimation</b><br><i>Matthew Joseph (University of Pennsylvania) &middot; Janardhan Kulkarni (Microsoft Research) &middot; Jieming Mao (Google Research) &middot; Steven Wu (Microsoft Research)</i></p>
<p><b>Individual Regret in Cooperative Nonstochastic Multi-Armed Bandits</b><br><i>Yogev Bar-On (Tel-Aviv University) &middot; Yishay Mansour (Tel Aviv University / Google)</i></p>
<p><b>Augmented Neural ODEs</b><br><i>Emilien Dupont (Oxford University) &middot; Arnaud Doucet (Oxford) &middot; Yee Whye Teh (University of Oxford, DeepMind)</i></p>
<p><b>DppNet: Approximating Determinantal Point Processes with Deep Networks</b><br><i>Zelda Mariet (MIT) &middot; Yaniv Ovadia (Google Inc) &middot; Jasper Snoek (Google Brain)</i></p>
<p><b>A Stickier Benchmark for General-Purpose Language Understanding Systems</b><br><i>Alex Wang (New York University) &middot; Yada Pruksachatkun (New York University) &middot; Nikita Nangia (NYU) &middot; Amanpreet Singh (Facebook) &middot; Julian Michael (University of Washington) &middot; Felix Hill (Google Deepmind) &middot; Omer Levy (Facebook) &middot; Samuel Bowman (New York University)</i></p>
<p><b>Transfusion: Understanding Transfer Learning for Medical Imaging</b><br><i>Maithra Raghu (Cornell University and Google Brain) &middot; Chiyuan Zhang (Google Brain) &middot; Jon Kleinberg (Cornell University) &middot; Samy Bengio (Google Research, Brain Team)</i></p>
<p><b>Adversarial training for free!</b><br><i>Ali Shafahi (University of Maryland) &middot; Mahyar Najibi (University of Maryland) &middot; Mohammad Amin Ghiasi (University of Maryland) &middot; Zheng Xu (Google AI) &middot; John P Dickerson (University of Maryland) &middot; Christoph Studer (Cornell University) &middot; Larry Davis (University of Maryland) &middot; Gavin Taylor (US Naval Academy) &middot; Tom Goldstein (University of Maryland)</i></p>
<p><b>Dance to Music</b><br><i>Hsin-Ying Lee (University of California, Merced) &middot; Xiaodong Yang (NVIDIA Research) &middot; Ming-Yu Liu (Nvidia Research) &middot; Ting-Chun Wang (NVIDIA) &middot; Yu-Ding Lu (UC Merced) &middot; Ming-Hsuan Yang (UC Merced / Google) &middot; Jan Kautz (NVIDIA)</i></p>
<p><b>Practical and Consistent Estimation of f-Divergences</b><br><i>Paul Rubenstein (MPI for IS) &middot; Olivier Bousquet (Google Brain (Zurich)) &middot; Josip Djolonga (Google Research, Brain Team) &middot; Carlos Riquelme (Google Brain) &middot; Ilya Tolstikhin (MPI for Intelligent Systems)</i></p>
<p><b>Learning dynamic semi-algebraic proofs</b><br><i>Alhussein Fawzi (DeepMind) &middot; Mateusz Malinowski (DeepMind) &middot; Hamza Fawzi (University of Cambridge) &middot; Omar Fawzi (ENS Lyon)</i></p>
<p><b>Efficient Graph Generation with Graph Recurrent Attention Networks</b><br><i>Renjie Liao (University of Toronto) &middot; Yujia Li (DeepMind) &middot; Yang Song (Stanford University) &middot; Shenlong Wang (University of Toronto) &middot; Will Hamilton (McGill) &middot; David Duvenaud (University of Toronto) &middot; Raquel Urtasun (Uber ATG) &middot; Richard Zemel (Vector Institute/University of Toronto)</i></p>
<p><b>Training Language GANs from Scratch</b><br><i>Cyprien de Masson d'Autume (Google DeepMind) &middot; Shakir Mohamed (DeepMind) &middot; Mihaela Rosca (Google DeepMind) &middot; Jack Rae (DeepMind, UCL)</i></p>
<p><b>A Geometric Perspective on Optimal Representations for Reinforcement Learning</b><br><i>Marc Bellemare (Google Brain) &middot; Will Dabney (DeepMind) &middot; Robert Dadashi-Tazehozi (Google Brain) &middot; Adrien Ali Taiga (Google) &middot; Pablo Samuel Castro (Google) &middot; Nicolas Le Roux (Google Brain) &middot; Dale Schuurmans (Google Inc.) &middot; Tor Lattimore (DeepMind) &middot; Clare Lyle (University of Oxford)</i></p>
<p><b>Game Design for Eliciting Distinguishable Behavior</b><br><i>Fan Yang (Carnegie Mellon University) &middot; Liu Leqi (Carnegie Mellon University) &middot; Yifan Wu (Carnegie Mellon University) &middot; Zachary Lipton (Carnegie Mellon University) &middot; Pradeep Ravikumar (Carnegie Mellon University) &middot; Tom M Mitchell (Carnegie Mellon University) &middot; William Cohen (Google AI)</i></p>
<p><b>When does label smoothing help?</b><br><i>Rafael Müller (Google Brain) &middot; Simon Kornblith (Google Brain) &middot; Geoffrey E Hinton (Google & University of Toronto)</i></p>
<p><b>Prior-Free Dynamic Auctions with Low Regret Buyers</b><br><i>Yuan Deng (Duke University) &middot; Jon Schneider (Google Research) &middot; Balasubramanian Sivan (Google Research)</i></p>
<p><b>Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces</b><br><i>Chuan Guo (Cornell University) &middot; Ali Mousavi (Google Brain) &middot; Xiang Wu (Google) &middot; Daniel Holtmann-Rice (Google Inc) &middot; Satyen Kale (Google) &middot; Sashank Reddi (Google) &middot; Sanjiv Kumar (Google Research)</i></p>
<p><b>MixMatch: A Holistic Approach to Semi-Supervised Learning</b><br><i>David Berthelot (Google Brain) &middot; Nicholas Carlini (Google) &middot; Ian Goodfellow (Google Brain) &middot; Nicolas Papernot () &middot; Avital Oliver (Google Brain) &middot; Colin A Raffel (Google Brain)</i></p>
<p><b>Weight Agnostic Neural Networks</b><br><i>Adam Gaier (Bonn-Rhein-Sieg University of Applied Sciences) &middot; David Ha (Google Brain)</i></p>
<p><b>Learning to Predict Without Looking Ahead: World Models Without Forward Prediction</b><br><i>Daniel Freeman (Google Brain) &middot; David Ha (Google Brain) &middot; Luke Metz (Google Brain)</i></p>
<p><b>Reducing the variance in online optimization by transporting past gradients</b><br><i>Sébastien Arnold (USC) &middot; Pierre-Antoine Manzagol (Google) &middot; Reza Harikandeh (UBC) &middot; Ioannis Mitliagkas (Mila & University of Montreal) &middot; Nicolas Le Roux (Google Brain)</i></p>
<p><b>Off-Policy Evaluation of Generalization for Deep Q-Learning in Binary Reward Tasks</b><br><i>Alexander Irpan (Google Brain) &middot; Kanishka Rao (Google) &middot; Konstantinos Bousmalis (DeepMind) &middot; Chris Harris (Google) &middot; Julian Ibarz (Google Inc.) &middot; Sergey Levine (Google)</i></p>
<p><b>Regularized Gradient Boosting</b><br><i>Corinna Cortes (Google Research) &middot; Mehryar Mohri (Courant Inst. of Math. Sciences & Google Research) &middot; Dmitry Storcheus (Google Research)</i></p>
<p><b>Invertible Convolutional Flow</b><br><i>Mahdi Karami (University of Alberta) &middot; Dale Schuurmans (Google) &middot; Jascha Sohl-Dickstein (Google Brain) &middot; Laurent Dinh (Google Research) &middot; Daniel Duckworth (Google Brain)</i></p>
<p><b>XLNet: Generalized Autoregressive Pretraining for Language Understanding</b><br><i>Zhilin Yang (Tsinghua University) &middot; Zihang Dai (Carnegie Mellon University) &middot; Yiming Yang (CMU) &middot; Jaime Carbonell (CMU) &middot; Ruslan Salakhutdinov (Carnegie Mellon University) &middot; Quoc V Le (Google)</i></p>
<p><b>Mixtape: Breaking the Softmax Bottleneck Efficiently</b><br><i>Zhilin Yang (Tsinghua University) &middot; Thang Luong (Google) &middot; Ruslan Salakhutdinov (Carnegie Mellon University) &middot; Quoc V Le (Google)</i></p>
<p><b>Learning GANs and Ensembles Using Discrepancy</b><br><i>Ben Adlam (Google) &middot; Corinna Cortes (Google Research) &middot; Mehryar Mohri (Courant Inst. of Math. Sciences & Google Research) &middot; Ningshan Zhang (NYU)</i></p>
<p><b>Abstract Reasoning with Distracting Features</b><br><i>Kecheng Zheng (University of Science and Technology of China) &middot; Zheng-Jun Zha (University of Science and Technology of China) &middot; Wei Wei (Google AI)</i></p>
<p><b>Direct Optimization through $rg \max$ for Discrete Variational Auto-Encoder</b><br><i>Guy Lorberbom (Technion) &middot; Tommi Jaakkola (MIT) &middot; Andreea Gane (Google AI) &middot; Tamir Hazan (Technion)</i></p>
<p><b>Interval timing in deep reinforcement learning agents</b><br><i>Ben Deverett (DeepMind) &middot; Ryan Faulkner (Deepmind) &middot; Meire Fortunato (DeepMind) &middot; Gregory Wayne (Google DeepMind) &middot; Joel Leibo (DeepMind)</i></p>
<p><b>Graph-based Discriminators: Sample Complexity and Expressiveness</b><br><i>Roi Livni (Tel Aviv University) &middot; Yishay Mansour (Tel Aviv University / Google)</i></p>
<p><b>Learning Nonsymmetric Determinantal Point Processes</b><br><i>Mike Gartrell (Criteo AI Lab) &middot; Victor-Emmanuel Brunel (ENSAE ParisTech) &middot; Elvis Dohmatob (Criteo) &middot; Syrine Krichene (Google)</i></p>
<p><b>Hypothesis Set Stability and Generalization</b><br><i>Dylan Foster (MIT) &middot; Spencer Greenberg (Spark Wave) &middot; Satyen Kale (Google) &middot; Haipeng Luo (University of Southern California) &middot; Mehryar Mohri (Courant Inst. of Math. Sciences & Google Research) &middot; Karthik Sridharan (Cornell University)</i></p>
<p><b>Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds</b><br><i>Bo Yang (University of Oxford) &middot; Jianan Wang (DeepMind) &middot; Ronald Clark (Imperial College London) &middot; Qingyong Hu (University of Oxford) &middot; Sen Wang (Heriot-Watt University) &middot; Andrew Markham (University of Oxford) &middot; Niki Trigoni (University of Oxford)</i></p>
<p><b>Differentiable Sorting using Optimal Transport: The Sinkhorn CDF and Quantile Operator</b><br><i>Marco Cuturi (Google and CREST/ENSAE) &middot; Olivier Teboul (Google Brain) &middot; Jean-Philippe Vert ()</i></p>
<p><b>Subspace Detours: Building Transport Plans that are Optimal on Subspace Projections</b><br><i>Boris Muzellec (ENSAE, Institut Polytechnique de Paris) &middot; Marco Cuturi (Google and CREST/ENSAE)</i></p>
<p><b>Neural Spline Flows</b><br><i>Conor Durkan (University of Edinburgh) &middot; Arturs Bekasovs (University of Edinburgh) &middot; Iain Murray (University of Edinburgh) &middot; George Papamakarios (DeepMind)</i></p>
<p><b>Continual Unsupervised Representation Learning</b><br><i>Dushyant Rao (DeepMind) &middot; Francesco Visin (DeepMind) &middot; Andrei Rusu (DeepMind) &middot; Razvan Pascanu (Google DeepMind) &middot; Yee Whye Teh (University of Oxford, DeepMind) &middot; Raia Hadsell (DeepMind)</i></p>
<p><b>Stochastic Runge-Kutta Accelerates Langevin Monte Carlo and Beyond</b><br><i>Xuechen Li (Google) &middot; Yi Wu (University of Toronto & Vector Institute) &middot; Lester Mackey (Microsoft Research) &middot; Murat Erdogdu (University of Toronto)</i></p>
<p><b>On two ways to use determinantal point processes for Monte Carlo integration</b><br><i>Guillaume Gautier (CNRS, INRIA, Univ. Lille) &middot; Rémi Bardenet (University of Lille) &middot; Michal Valko (DeepMind Paris and Inria Lille - Nord Europe)</i></p>
<p><b>Detecting Overfitting via Adversarial Examples</b><br><i>Roman Werpachowski (DeepMind) &middot; András György (DeepMind) &middot; Csaba Szepesvari (DeepMind/University of Alberta)</i></p>
<p><b>SMILe: Scalable Meta Inverse Reinforcement Learning through Context-Conditional Policies</b><br><i>Seyed Kamyar Seyed Ghasemipour (University of Toronto) &middot; Shixiang (Shane) Gu (Google Brain) &middot; Richard Zemel (Vector Institute/University of Toronto)</i></p>
<p><b>Differentially Private Anonymized Histograms</b><br><i>Ananda Theertha Suresh (Google)</i></p>
<p><b>PyTorch: An Imperative Style, High-Performance Deep Learning Library</b><br><i>Benoit Steiner (Facebook AI Research) &middot; Zachary DeVito (Facebook AI Research) &middot; Soumith Chintala (Facebook AI Research) &middot; Sam Gross (Facebook) &middot; Adam Paszke (University of Warsaw) &middot; Francisco Massa (Facebook AI Research) &middot; Adam Lerer (Facebook AI Research) &middot; Gregory Chanan (Facebook) &middot; Zeming Lin (Facebook AI Research) &middot; Edward Yang (Facebook) &middot; Alban Desmaison (Oxford University) &middot; Alykhan Tejani (Twitter, Inc.) &middot; Andreas Kopf (Xamla) &middot; James Bradbury (Google Brain) &middot; Luca Antiga (Orobix) &middot; Martin Raison (Nabla) &middot; Natalia Gimelshein (NVIDIA) &middot; Sasank Chilamkurthy (Qure.ai) &middot; Trevor Killeen (Self Employed) &middot; Lu Fang (Facebook) &middot; Junjie Bai (Facebook)</i></p>
<p><b>Fast Convergence of Natural Gradient Descent for Over-Parameterized Neural Networks</b><br><i>Guodong Zhang (University of Toronto) &middot; James Martens (DeepMind) &middot; Roger Grosse (University of Toronto)</i></p>
<p><b>Which Algorithmic Choices Matter at Which Batch Sizes?  Insights From a Noisy Quadratic Model</b><br><i>Guodong Zhang (University of Toronto) &middot; Lala Li (Google) &middot; Zachary Nado (Google Inc.) &middot; James Martens (DeepMind) &middot; Sushant Sachdeva (University of Toronto) &middot; George Dahl (Google Brain) &middot; Chris Shallue (Google Brain) &middot; Roger Grosse (University of Toronto)</i></p>
<p><b>Spherical Text Embedding</b><br><i>Yu Meng (University of Illinois at Urbana-Champaign) &middot; Jiaxin Huang (University of Illinois Urbana-Champaign) &middot; Guangyuan Wang (UIUC) &middot; Chao Zhang (Georgia Institute of Technology) &middot; Honglei Zhuang (Google Research) &middot; Lance Kaplan (U.S. Army Research Laboratory) &middot; Jiawei Han (UIUC)</i></p>
<p><b>Revisiting Auxiliary Latent Variables in Generative Models</b><br><i>John Lawson (New York University) &middot; George Tucker (Google Brain) &middot; Bo Dai (Google Brain) &middot; Rajesh Ranganath (New York University)</i></p>
<p><b>Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent</b><br><i>Jaehoon Lee (Google Brain) &middot; Lechao Xiao (Google Brain) &middot; Samuel Schoenholz (Google Brain) &middot; Yasaman Bahri (Google Brain) &middot; Roman Novak (Google Brain) &middot; Jascha Sohl-Dickstein (Google Brain) &middot; Jeffrey Pennington (Google Brain)</i></p>
<p><b>Visualizing and Measuring the Geometry of BERT</b><br><i>Emily Reif (Google) &middot; Ann Yuan (Google) &middot; Martin Wattenberg (Google) &middot; Fernanda B Viegas (Google) &middot; Andy Coenen (Google) &middot; Adam Pearce (Google) &middot; Been Kim (Google)</i></p>
<p><b>Learning to Screen</b><br><i>Alon Cohen (Technion and Google Inc.) &middot; Avinatan Hassidim (Google) &middot; Haim Kaplan (TAU, GOOGLE) &middot; Yishay Mansour (Tel Aviv University / Google) &middot; Shay Moran (IAS, Princeton)</i></p>
<p><b>A Robust Non-Clairvoyant Dynamic Mechanism for Contextual Auctions</b><br><i>Yuan Deng (Duke University) &middot; Sebastien Lahaie (Google Research) &middot; Vahab Mirrokni (Google Research NYC)</i></p>
<p><b>Graph Agreement Models for Semi-Supervised Learning</b><br><i>Otilia Stretcu (Carnegie Mellon University) &middot; Krishnamurthy Viswanathan (Google Research) &middot; Dana Movshovitz-Attias (Google) &middot; Emmanouil Platanios (Carnegie Mellon University) &middot; Sujith Ravi (Google Research) &middot; Andrew Tomkins (Google)</i></p>
<p><b>Surrogate Objectives for Batch Policy Optimization in One-step Decision Making</b><br><i>Minmin Chen (Google) &middot; Ramki Gummadi (Google) &middot; Chris Harris (Google) &middot; Dale Schuurmans (University of Alberta & Google Brain)</i></p>
<p><b>Retrosynthesis Prediction with Conditional Graph Logic Network</b><br><i>Hanjun Dai (Georgia Tech) &middot; Chengtao Li (MIT) &middot; Connor Coley (MIT) &middot; Bo Dai (Google Brain) &middot; Le Song (Ant Financial & Georgia Institute of Technology)</i></p>
<p><b>Reconciling meta-learning and continual learning with online mixtures of tasks</b><br><i>Ghassen Jerfel (Duke University) &middot; Erin Grant (UC Berkeley) &middot; Thomas Griffiths (Princeton University) &middot; Katherine Heller (Google)</i></p>
<p><b>Towards Automatic Concept-based Explanations</b><br><i>Amirata Ghorbani (Stanford University) &middot; James Wexler () &middot; James Zou (Stanford University) &middot; Been Kim (Google)</i></p>
<p><b>Budgeted Reinforcement Learning in Continuous State Space</b><br><i>Nicolas Carrara (inria) &middot; Edouard Leurent (INRIA) &middot; Romain Laroche (Microsoft Research) &middot; Tanguy Urvoy (Orange-Labs) &middot; Odalric-Ambrym Maillard (INRIA) &middot; Olivier Pietquin (Google Research    Brain Team)</i></p>
<p><b>The Discovery of Useful Questions as Auxiliary Tasks</b><br><i>Vivek Veeriah (University of Michigan) &middot; Richard L Lewis (University of Michigan) &middot; Janarthanan Rajendran (University of Michigan) &middot; David Silver (DeepMind) &middot; Satinder Singh (University of Michigan)</i></p>
<p><b>Understanding Posterior Collapse in Variational Autoencoders</b><br><i>James Lucas (University of Toronto) &middot; George Tucker (Google Brain) &middot; Roger Grosse (University of Toronto) &middot; Mohammad Norouzi (Google Brain)</i></p>
<p><b>Language as an Abstraction for Hierarchical Deep Reinforcement Learning</b><br><i>YiDing Jiang (Google) &middot; Shixiang (Shane) Gu (Google Brain) &middot; Kevin P Murphy (Google) &middot; Chelsea Finn (Google Brain)</i></p>
<p><b>Tight Dimensionality Reduction for Sketching Low Degree Polynomial Kernels</b><br><i>Michela Meister (Google) &middot; Tamas Sarlos (Google Research) &middot; David Woodruff (Carnegie Mellon University)</i></p>
<p><b>Maximum Entropy Monte-Carlo Planning</b><br><i>Chenjun Xiao (University of Alberta) &middot; Ruitong Huang (Borealis AI) &middot; Jincheng Mei (University of Alberta) &middot; Dale Schuurmans (Google) &middot; Martin Müller (University of Alberta)</i></p>
<p><b>Random Tessellation Forests</b><br><i>Shufei Ge (Simon Fraser University) &middot; Shijia Wang (Simon Fraser University) &middot; Yee Whye Teh (University of Oxford, DeepMind) &middot; Liangliang Wang (Simon Fraser University) &middot; Lloyd T Elliott (Simon Fraser University)</i></p>
<p><b>Lookahead Optimizer: k steps forward, 1 step back</b><br><i>Michael Zhang (University of Toronto) &middot; James Lucas (University of Toronto) &middot; Jimmy Ba (University of Toronto / Vector Institute) &middot; Geoffrey Hinton (Google)</i></p>
<p><b>Contextual Bandits with Cross-Learning</b><br><i>Santiago Balseiro (Columbia University) &middot; Negin Golrezaei (University of Southern California) &middot; Mohammad Mahdian (Google Research) &middot; Vahab Mirrokni (Google Research NYC) &middot; Jon Schneider (Google Research)</i></p>
<p><b>A Benchmark for Interpretability Methods in Deep Neural Networks</b><br><i>Sara Hooker (Google AI Resident) &middot; Dumitru Erhan (Google Brain) &middot; Pieter-Jan Kindermans (Google Brain) &middot; Been Kim (Google)</i></p>
<p><b>Memory Efficient Adaptive Optimization</b><br><i>Rohan Anil (Google) &middot; Vineet Gupta (Google) &middot; Tomer Koren (Google) &middot; Yoram Singer (Google)</i></p>
<p><b>Dynamic Incentive-Aware Learning: Robust Pricing in Contextual Auctions</b><br><i>Negin Golrezaei (MIT) &middot; Adel Javanmard (USC) &middot; Vahab Mirrokni (Google Research NYC)</i></p>
<p><b>A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning</b><br><i>Xuanqing Liu (University of California, Los Angeles) &middot; Si Si (Google Research) &middot; Jerry Zhu (University of Wisconsin-Madison) &middot; Yang Li (Google) &middot; Cho-Jui Hsieh (UCLA)</i></p>
<p><b>Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond</b><br><i>Lin Chen (Yale University) &middot; Hossein Esfandiari (Google Research) &middot; Gang Fu (Google Inc) &middot; Vahab Mirrokni (Google Research NYC)</i></p>
<p><b>Logarithmic Regret for Online Control</b><br><i>Naman Agarwal (Google) &middot; Elad Hazan (Princeton University) &middot; Karan Singh (Princeton University)</i></p>
<p><b>From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization</b><br><i>Krzysztof M Choromanski (Google Brain Robotics) &middot; Aldo Pacchiano (UC Berkeley) &middot; Jack Parker-Holder (Columbia University) &middot; Yunhao Tang (Columbia University) &middot; Vikas Sindhwani (Google)</i></p>
<p><b>Bandits with Feedback Graphs and Switching Costs</b><br><i>Raman Arora (Johns Hopkins University) &middot; Teodor Vanislavov Marinov (Johns Hopkins University) &middot; Mehryar Mohri (Courant Inst. of Math. Sciences & Google Research)</i></p>
<p><b>Nearly Tight Bounds for Robust Proper Learning of Halfspaces with a Margin</b><br><i>Ilias Diakonikolas (USC) &middot; Daniel Kane (UCSD) &middot; Pasin Manurangsi (Google)</i></p>
<p><b>Large Scale Adversarial Representation Learning</b><br><i>Jeff Donahue (DeepMind) &middot; Karen Simonyan (DeepMind)</i></p>
<p><b>Multilabel reductions: what is my loss optimising?</b><br><i>Aditya Menon (Google) &middot; Ankit Singh Rawat (Google Research) &middot; Sashank Reddi (Google) &middot; Sanjiv Kumar (Google Research)</i></p>
<p><b>Unsupervised Learning of Object Keypoints for Perception and Control</b><br><i>Tejas Kulkarni (DeepMind) &middot; Ankush Gupta (DeepMind) &middot; Catalin Ionescu (Deepmind) &middot; Sebastian Borgeaud (DeepMind) &middot; Malcolm Reynolds (DeepMind) &middot; Andrew Zisserman (DeepMind & University of Oxford) &middot; Volodymyr Mnih (DeepMind)</i></p>
<p><b>Optimizing Generalized Rate Metrics through Three-player Games</b><br><i>Harikrishna Narasimhan (Google) &middot; Andrew Cotter (Google) &middot; Maya Gupta (Google)</i></p>
<p><b>On Making Stochastic Classifiers Deterministic</b><br><i>Andrew Cotter (Google) &middot; Maya Gupta (Google) &middot; Harikrishna Narasimhan (Google)</i></p>
<p><b>Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks</b><br><i>Sitao Luan (McGill University) &middot; Mingde Zhao (Mila, McGill University) &middot; Xiao-Wen Chang (McGill University) &middot; Doina Precup (McGill University / DeepMind Montreal)</i></p>
<p><b>Exponential Family Estimation via Adversarial Dynamics Embedding</b><br><i>Bo Dai (Google Brain) &middot; Zhen Liu (Georgia Institute of Technology) &middot; Hanjun Dai (Georgia Institute of Technology) &middot; Niao He (UIUC) &middot; Arthur Gretton (Gatsby Unit, UCL) &middot; Le Song (Ant Financial & Georgia Institute of Technology) &middot; Dale Schuurmans (Google Inc.)</i></p>
<p><b>On Distributed Averaging for Stochastic k-PCA</b><br><i>Aditya Bhaskara (Google Research) &middot; Pruthuvi Wijewardena (University of Utah)</i></p>
<p><b>Greedy Sampling for Approximate Clustering in the Presence of Outliers</b><br><i>Aditya Bhaskara (Google Research) &middot; Sharvaree Vadgama (University of Utah) &middot; Hong Xu (University of Utah)</i></p>
<p><b>Meta Architecture Search</b><br><i>Albert Shaw (Deepscale) &middot; Wei Wei (Google AI) &middot; Weiyang Liu (Georgia Institute of Technology) &middot; Le Song (Ant Financial & Georgia Institute of Technology) &middot; Bo Dai (Google Brain)</i></p>
<p><b>Private Stochastic Convex Optimization with Optimal Rates</b><br><i>Raef Bassily (The Ohio State University) &middot; Vitaly Feldman (Google Brain) &middot; Kunal Talwar (Google) &middot; Abhradeep Guha Thakurta (University of California Santa Cruz)</i></p>
<p><b>Exact sampling of determinantal point processes with sublinear time preprocessing</b><br><i>Michal Derezinski (UC Berkeley) &middot; Daniele Calandriello (LCSL IIT/MIT) &middot; Michal Valko (DeepMind Paris and Inria Lille - Nord Europe)</i></p>
<p><b>Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction</b><br><i>Aviral Kumar (UC Berkeley) &middot; Justin Fu (UC Berkeley) &middot; Matthew Soh (UC Berkeley) &middot; George Tucker (Google Brain) &middot; Sergey Levine (UC Berkeley)</i></p>
<p><b>Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates</b><br><i>Carlos Riquelme (Google Brain) &middot; Hugo Penedones (Google DeepMind) &middot; Damien Vincent (Google Brain) &middot; Hartmut Maennel (Google) &middot; Sylvain Gelly (Google Brain (Zurich)) &middot; Timothy A Mann (DeepMind) &middot; Andre Barreto (DeepMind) &middot; Gergely Neu (Universitat Pompeu Fabra)</i></p>
<p><b>Connections Between Mirror Descent, Thompson Sampling and the Information Ratio</b><br><i>Julian Zimmert (University of Copenhagen) &middot; Tor Lattimore (DeepMind)</i></p>
<p><b>Adaptive Density Estimation for Generative Models</b><br><i>Thomas LUCAS (Inria Grenoble) &middot; Konstantin Shmelkov (Huawei) &middot; Karteek Alahari (Inria) &middot; Cordelia Schmid (Inria / Google) &middot; Jakob Verbeek (INRIA)</i></p>
<p><b>Weighted Linear Bandits for Non-Stationary Environments</b><br><i>Yoan Russac (Ecole Normale Supérieure) &middot; Claire Vernade (Google DeepMind) &middot; Olivier Cappé (CNRS)</i></p>
<p><b>A Domain Agnostic Measure for Monitoring and Evaluating GANs</b><br><i>Paulina Grnarova (ETH Zurich) &middot; Yehuda Kfir Levy (ETH) &middot; Aurelien Lucchi (ETH Zurich) &middot; Nathanael Perraudin (Swiss Data Science Center - EPFL / ETH Zurich) &middot; Ian Goodfellow (Google) &middot; Thomas Hofmann (ETH Zurich) &middot; Andreas Krause (ETH Zurich)</i></p>
<p><b>Integer Discrete Flows and Lossless Compression</b><br><i>Emiel Hoogeboom (University of Amsterdam) &middot; Jorn Peters (University of Amsterdam) &middot; Rianne van den Berg (Google Brain) &middot; Max Welling (University of Amsterdam / Qualcomm AI Research)</i></p>
<p><b>Are Labels Required for Improving Adversarial Robustness?</b><br><i>Jean-Baptiste Alayrac (Deepmind) &middot; Jonathan Uesato (DeepMind) &middot; Po-Sen Huang (DeepMind) &middot; Alhussein Fawzi (DeepMind) &middot; Robert Stanforth (DeepMind) &middot; Pushmeet Kohli (DeepMind)</i></p>
<p><b>Think out of the &quot;Box&quot;: Generically-Constrained Asynchronous Composite Optimization and Hedging</b><br><i>Pooria Joulani (DeepMind) &middot; András György (DeepMind) &middot; Csaba Szepesvari (DeepMind/University of Alberta)</i></p>
<p><b>Classification Accuracy Score for Conditional Generative Models</b><br><i>Suman Ravuri (DeepMind) &middot; Oriol Vinyals (Google DeepMind)</i></p>
<p><b>Multiagent Evaluation under Incomplete Information</b><br><i>Mark Rowland (DeepMind) &middot; Shayegan Omidshafiei (DeepMind) &middot; Karl Tuyls (DeepMind) &middot; Julien Perolat (DeepMind) &middot; Michal Valko (DeepMind Paris and Inria Lille - Nord Europe) &middot; Georgios Piliouras (Singapore University of Technology and Design) &middot; Remi Munos (DeepMind)</i></p>
<p><b>Tree-Sliced Variants of Wasserstein Distances</b><br><i>Tam Le (RIKEN AIP) &middot; Makoto Yamada (Kyoto University / RIKEN AIP) &middot; Kenji Fukumizu (Institute of Statistical Mathematics / Preferred Networks / RIKEN AIP) &middot; Marco Cuturi (Google and CREST/ENSAE)</i></p>
<p><b>Robustness Verification of Tree-based Models</b><br><i>Hongge Chen (MIT) &middot; Huan Zhang (UCLA) &middot; Si Si (Google Research) &middot; Yang Li (Google) &middot; Duane Boning (Massachusetts Institute of Technology) &middot; Cho-Jui Hsieh (UCLA)</i></p>
<p><b>Towards Interpretable Reinforcement Learning Using Attention Augmented Agents</b><br><i>Alexander Mott (DeepMind) &middot; Daniel Zoran (DeepMind) &middot; Mike Chrzanowski (DeepMind) &middot; Daan Wierstra (DeepMind Technologies) &middot; Danilo Jimenez Rezende (Google DeepMind)</i></p>
<p><b>Planning in Entropy-Regularized Markov Decision Processes and Games</b><br><i>Jean-Bastien Grill (Google DeepMind) &middot; Omar Darwiche Domingues (Inria) &middot; Pierre Menard (Inria) &middot; Remi Munos (DeepMind) &middot; Michal Valko (DeepMind Paris and Inria Lille - Nord Europe)</i></p>
<p><b>Generalization of Reinforcement Learners with Working and Episodic Memory</b><br><i>Meire Fortunato (DeepMind) &middot; Melissa Tan (Deepmind) &middot; Ryan Faulkner (Deepmind) &middot; Steven Hansen (DeepMind) &middot; Adrià Puigdomènech Badia (Google DeepMind) &middot; Gavin Buttimore (DeepMind) &middot; Charles Deck (Deepmind) &middot; Joel Leibo (DeepMind) &middot; Charles Blundell (DeepMind)</i></p>
<p><b>Hindsight Credit Assignment</b><br><i>Anna Harutyunyan (DeepMind) &middot; Will Dabney (DeepMind) &middot; Thomas Mesnard (DeepMind) &middot; Mohammad Gheshlaghi Azar (DeepMind) &middot; Bilal Piot (DeepMind) &middot; Nicolas Heess (Google DeepMind) &middot; Hado van Hasselt (DeepMind) &middot; Gregory Wayne (Google DeepMind) &middot; Satinder Singh (DeepMind) &middot; Doina Precup (DeepMind) &middot; Remi Munos (DeepMind)</i></p>
<p><b>Continuous Hierarchical Representations with Poincaré Variational Auto-Encoders </b><br><i>Emile Mathieu () &middot; Charline Le Lan (University of Oxford) &middot; Chris J. Maddison (Institute for Advanced Study, Princeton) &middot; Ryota Tomioka (Microsoft Research Cambridge) &middot; Yee Whye Teh (University of Oxford, DeepMind)</i></p>
<p><b>MetaInit: Initializing learning by learning to initialize</b><br><i>Yann Dauphin (Google AI) &middot; Samuel Schoenholz (Google Brain)</i></p>
<p><b>Sim2real transfer learning for 3D pose estimation: motion to the rescue</b><br><i>Carl Doersch (DeepMind) &middot; Andrew Zisserman (DeepMind & University of Oxford)</i></p>
<p><b>Generalization Bounds for Neural Networks via Approximate Description Length</b><br><i>Amit Daniely (Google Research) &middot; Elad Granot (Hebrew University)</i></p>
<p><b>The Option Keyboard: Combining Skills in Reinforcement Learning</b><br><i>Andre Barreto (DeepMind) &middot; Diana Borsa (DeepMind) &middot; Shaobo Hou (DeepMind) &middot; Gheorghe Comanici (Google) &middot; Eser Aygun (Google Canada) &middot; Philippe Hamel (Google) &middot; Daniel Toyama (DeepMind Montreal) &middot; Jonathan J Hunt (DeepMind) &middot; Shibl Mourad (Google) &middot; David Silver (DeepMind) &middot; Doina Precup (DeepMind)</i></p>
<p><b>Biases for Emergent Communication in Multi-agent Reinforcement Learning</b><br><i>Tom Eccles (DeepMind) &middot; Yoram Bachrach () &middot; Guy Lever (Google DeepMind) &middot; Angeliki Lazaridou (DeepMind) &middot; Thore Graepel (DeepMind)</i></p>
<p><b>Episodic Memory in Lifelong Language Learning</b><br><i>Cyprien de Masson d'Autume (Google DeepMind) &middot; Sebastian Ruder (DeepMind) &middot; Lingpeng Kong (DeepMind) &middot; Dani Yogatama (DeepMind)</i></p>
<p><b>A Fourier Perspective on Model Robustness in Computer Vision</b><br><i>Dong Yin (UC Berkeley) &middot; Raphael Gontijo Lopes (Google Brain) &middot; Ekin Dogus Cubuk (Google Brain) &middot; Justin Gilmer (Google Brain) &middot; Jon Shlens (Google Research)</i></p>
<p><b>Variance Reduction in Bipartite Experiments through Correlation Clustering</b><br><i>Jean Pouget-Abadie (Harvard University) &middot; Kevin Aydin (Google) &middot; Warren Schudy (Google) &middot; Kay Brodersen (Google) &middot; Vahab Mirrokni (Google Research NYC)</i></p>
<p><b>Shaping Belief States with Generative Environment Models for RL</b><br><i>Karol Gregor (DeepMind) &middot; Danilo Jimenez Rezende (Google DeepMind) &middot; Frederic Besse (DeepMind) &middot; Yan Wu (DeepMind) &middot; Hamza Merzic (Deepmind) &middot; Aaron van den Oord (Google Deepmind)</i></p>
<p><b>Globally Optimal Learning for Structured Elliptical Losses</b><br><i>Yoav Wald (Hebrew University) &middot; Nofar Noy (Hebrew University) &middot; Gal Elidan (Google) &middot; Ami Wiesel (Google Research and The Hebrew University of Jerusalem, Israel)</i></p>
<p><b>Graph Normalizing Flows</b><br><i>Jenny Liu (University of Toronto) &middot; Aviral Kumar (UC Berkeley) &middot; Jimmy Ba (University of Toronto / Vector Institute) &middot; Jamie Kiros (Google Inc.) &middot; Kevin Swersky (Google)</i></p>
<p><b>On Robustness to Adversarial Examples and Polynomial Optimization</b><br><i>Pranjal Awasthi (Rutgers University/Google) &middot; Abhratanu Dutta (Northwestern University) &middot; Aravindan Vijayaraghavan (Northwestern University)</i></p>
<p><b>Adversarial Robustness through Local Linearization</b><br><i>Chongli Qin (DeepMind) &middot; James Martens (DeepMind) &middot; Sven Gowal (DeepMind) &middot; Dilip Krishnan (Google) &middot; Krishnamurthy Dvijotham (DeepMind) &middot; Alhussein Fawzi (DeepMind) &middot; Soham De (DeepMind) &middot; Robert Stanforth (DeepMind) &middot; Pushmeet Kohli (DeepMind)</i></p>
<p><b>Sampled softmax with random Fourier features</b><br><i>Ankit Singh Rawat (Google Research) &middot; Jiecao Chen (Indiana University Bloomington) &middot; Felix Xinnan Yu (Google Research) &middot; Ananda Theertha Suresh (Google) &middot; Sanjiv Kumar (Google Research)</i></p>
<p><b>Can you trust your model&#39;s uncertainty?  Evaluating predictive uncertainty under dataset shift</b><br><i>Jasper Snoek (Google Brain) &middot; Yaniv Ovadia (Google Inc) &middot; Emily Fertig (Google Brain) &middot; Balaji Lakshminarayanan (Google DeepMind) &middot; Sebastian Nowozin (Google Research) &middot; D. Sculley (Google Research) &middot; Joshua Dillon (Google) &middot; Jie Ren (Google Inc.) &middot; Zachary Nado (Google Inc.)</i></p>
<p><b>Variational Bayesian Optimal Experimental Design</b><br><i>Adam Foster (University of Oxford) &middot; Martin Jankowiak (Uber AI Labs) &middot; Eli Bingham (Uber AI Labs) &middot; Paul Horsfall (Uber AI Labs) &middot; Yee Whye Teh (University of Oxford, DeepMind) &middot; Tom Rainforth (University of Oxford) &middot; Noah Goodman (Stanford University)</i></p>
<p><b>Differentially Private Covariance Estimation</b><br><i>Kareem Amin (Google Research) &middot; Travis Dick (Carnegie Mellon University) &middot; Alex Kulesza (Google) &middot; Andres Munoz (Google) &middot; Sergei Vassilvitskii (Google)</i></p>
<p><b>Are Disentangled Representations Helpful for Abstract Visual Reasoning?</b><br><i>Sjoerd van Steenkiste (The Swiss AI Lab - IDSIA) &middot; Francesco Locatello (ETH Zürich - MPI Tübingen) &middot; Jürgen Schmidhuber (Swiss AI Lab, IDSIA (USI & SUPSI) - NNAISENSE) &middot; Olivier Bachem (Google Brain)</i></p>
<p><b>Robust Attribution Regularization</b><br><i>Jiefeng Chen (University of Wisconsin-Madison) &middot; Xi Wu (Google) &middot; Vaibhav Rastogi (University of Wisconsin-Madison) &middot; Yingyu Liang (University of Wisconsin Madison) &middot; Somesh Jha (University of Wisconsin, Madison)</i></p>
<p><b>Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization</b><br><i>Miika Aittala (MIT) &middot; Prafull Sharma (MIT) &middot; Lukas Murmann (Massachusetts Institute of Technology) &middot; Adam Yedidia (Massachusetts Institute of Technology) &middot; Gregory Wornell (MIT) &middot; Bill Freeman (MIT/Google) &middot; Fredo Durand (MIT)</i></p>
<p><b>When to use parametric models in reinforcement learning?</b><br><i>Hado van Hasselt (DeepMind) &middot; Matteo Hessel (Google DeepMind) &middot; John Aslanides (DeepMind)</i></p>
<p><b>Hamiltonian descent for composite objectives</b><br><i>Brendan O'Donoghue (Google DeepMind) &middot; Chris J. Maddison (Institute for Advanced Study, Princeton)</i></p>
<p><b>On the Benefits of Disentangled Representations</b><br><i>Francesco Locatello (ETH Zürich - MPI Tübingen) &middot; Gabriele Abbati (University of Oxford) &middot; Tom Rainforth (University of Oxford) &middot; Stefan Bauer (MPI for Intelligent Systems) &middot; Bernhard Schölkopf (MPI for Intelligent Systems) &middot; Olivier Bachem (Google Brain)</i></p>
<p><b>Bayesian Layers: A Module for Neural Network Uncertainty</b><br><i>Dustin Tran (Google Brain) &middot; Mike Dusenberry (Google Brain) &middot; Mark van der Wilk (PROWLER.io) &middot; Danijar Hafner (Google)</i></p>
<p><b>Learning Compositional Neural Programs with Recursive Tree Search and Planning</b><br><i>Thomas PIERROT (InstaDeep) &middot; Guillaume Ligner (InstaDeep) &middot; Scott Reed (Google DeepMind) &middot; Olivier Sigaud (Sorbonne University) &middot; Perrin Nicolas (ISIR) &middot; David Kas (InstaDeep) &middot; David Kas (InstaDeep) &middot; Karim Beguir (InstaDeep) &middot; Nando de Freitas (DeepMind)</i></p>
<p><b>Likelihood Ratios for Out-of-Distribution Detection</b><br><i>Jie Ren (Google Brain) &middot; Peter Liu (Google Brain) &middot; Emily Fertig (Google Brain) &middot; Jasper Snoek (Google Brain) &middot; Ryan  Poplin (Google) &middot; Mark Depristo (Google) &middot; Joshua Dillon (Google) &middot; Balaji Lakshminarayanan (Google DeepMind)</i></p>
<p><b>Discrete Flows: Invertible Generative Models of Discrete Data</b><br><i>Dustin Tran (Google Brain) &middot; Keyon Vafa (Columbia University) &middot; Kumar Agrawal (Google AI Resident) &middot; Laurent Dinh (Google Research) &middot; Ben Poole (Google Brain)</i></p>
<p><b>Generating Diverse High-Fidelity Images with VQVAE-2</b><br><i>Ali Razavi (DeepMind) &middot; Aaron van den Oord (Google Deepmind) &middot; Oriol Vinyals (Google DeepMind)</i></p>
<p><b>Locally Private Learning without Interaction Requires Separation</b><br><i>Amit Daniely (Google Research) &middot; Vitaly Feldman (Google Brain)</i></p>
<p><b>Robust Bi-Tempered Logistic Loss Based on Bregman Divergences</b><br><i>Ehsan Amid (University of California, Santa Cruz) &middot; Manfred Warmuth (Univ. of Calif. at Santa Cruz) &middot; Rohan Anil (Google) &middot; Tomer Koren (Google)</i></p>
<p><b>Computational Separations between Sampling and Optimization</b><br><i>Kunal Talwar (Google)</i></p>
<p><b>Efficient Rematerialization for Deep Networks</b><br><i>Ravi Kumar (Google) &middot; Manish Purohit (Google) &middot; Zoya Svitkina (Google) &middot; Erik Vee (Google) &middot; Joshua Wang (Google)</i></p>
<p><b>Momentum-Based Variance Reduction in Non-Convex SGD</b><br><i>Ashok Cutkosky (Google Research) &middot; Francesco Orabona (Boston University)</i></p>
<p><b>Flattening a Hierarchical Clustering through Active Learning</b><br><i>Fabio Vitale (Sapienza University of Rome) &middot; Anand Rajagopalan (Google) &middot; Claudio Gentile (Google Research)</i></p>
<p><b>Kernel Truncated Randomized Ridge Regression: Optimal Rates and Low Noise Acceleration</b><br><i>Kwang-Sung Jun (Boston University) &middot; Ashok Cutkosky (Google Research) &middot; Francesco Orabona (Boston University)</i></p>
<p><b>Hamiltonian Neural Networks</b><br><i>Samuel Greydanus (Google Brain) &middot; Misko Dzumba (PetCube) &middot; Jason Yosinski (Uber AI Labs)</i></p>
<p><b>A Kernel Loss for Solving the Bellman Equation</b><br><i>Yihao Feng (The University of Texas at Austin) &middot; Lihong Li (Google Brain) &middot; Qiang Liu (UT Austin)</i></p>
<p><b>Stacked Capsule Autoencoders</b><br><i>Adam Kosiorek (University of Oxford) &middot; Sara Sabour (Google) &middot; Yee Whye Teh (University of Oxford, DeepMind) &middot; Geoffrey E Hinton (Google & University of Toronto)</i></p>
<p><b>Wasserstein Dependency Measure for Representation Learning</b><br><i>Sherjil Ozair (Université de Montréal) &middot; Corey Lynch (Google Brain) &middot; Yoshua Bengio (Mila) &middot; Aaron van den Oord (Google Deepmind) &middot; Sergey Levine (UC Berkeley) &middot; Pierre Sermanet (Google Brain)</i></p>
<p><b>Universality and individuality in neural dynamics across large populations of recurrent networks</b><br><i>Niru Maheswaranathan (Google Brain) &middot; Alex H Williams (Stanford University) &middot; Matthew Golub (Stanford University) &middot; Surya Ganguli (Stanford) &middot; David Sussillo (Google Inc.)</i></p>
<p><b>Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics</b><br><i>Niru Maheswaranathan (Google Brain) &middot; Alex H Williams (Stanford University) &middot; Matthew Golub (Stanford University) &middot; Surya Ganguli (Stanford) &middot; David Sussillo (Google Inc.)</i></p>
<p><b>Gradient-based Adaptive Markov Chain Monte Carlo</b><br><i>Michalis Titsias (DeepMind) &middot; Petros Dellaportas (University College London, Athens University of Economics and Alan Turing Institute)</i></p>
<p><b>On the Role of Inductive Bias From Simulation and the Transfer to the Real World: a new Disentanglement Dataset</b><br><i>Muhammad Waleed Gondal (Max Planck Institute for Intelligent Systems) &middot; Manuel Wuthrich (Max Planck Institute for Intelligent Systems) &middot; Djordje Miladinovic (ETH Zurich) &middot; Francesco Locatello (ETH Zürich - MPI Tübingen) &middot; Martin  Breidt (MPI for Biological Cybernetics) &middot; Valentin Volchkov (Max Planck Institut for Intelligent Systems) &middot; Joel Akpo (Max Planck Institute for Intelligent Systems) &middot; Olivier Bachem (Google Brain) &middot; Bernhard Schölkopf (MPI for Intelligent Systems) &middot; Stefan Bauer (MPI for Intelligent Systems)</i></p>
